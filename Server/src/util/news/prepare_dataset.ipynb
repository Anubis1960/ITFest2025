{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.utils import compute_class_weight\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from tensorflow.python.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.python.keras.saving.save import load_model\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def shuffle_data(data_frame):\n",
    "    return data_frame.sample(frac=1).reset_index(drop=True)\n"
   ],
   "id": "f38bca0af264842d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clear_dataset():\n",
    "    # Define the input and output file paths\n",
    "    input_file = 'news_cleaned_2018_02_13.csv'\n",
    "    output_file = 'news_cleaned_valid_records.csv'\n",
    "\n",
    "    # Define the chunk size (number of rows per chunk)\n",
    "    chunk_size = 1_000_000\n",
    "\n",
    "    csv.field_size_limit(131072 * 10)\n",
    "\n",
    "    # Initialize a flag to write the header only once\n",
    "    write_header = True\n",
    "\n",
    "    # Open the output file in append mode\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        # Read the input file in chunks\n",
    "        for chunk in pd.read_csv(\n",
    "            input_file,\n",
    "            usecols=['type', 'content', 'title'],\n",
    "            on_bad_lines='skip',\n",
    "            quoting=csv.QUOTE_MINIMAL,\n",
    "            engine='python',\n",
    "            chunksize=chunk_size\n",
    "        ):\n",
    "            try:\n",
    "                print(f\"Processing chunk with {len(chunk)} rows\")\n",
    "                # Drop rows with missing values in 'content', 'type', or 'title'\n",
    "                chunk_cleaned = chunk.dropna(subset=[\"content\", \"type\"])\n",
    "                chunk_cleaned = shuffle_data(chunk_cleaned)\n",
    "                # Write the cleaned chunk to the output file\n",
    "                chunk_cleaned.to_csv(f_out, index=False, header=write_header, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "                # After the first chunk, do not write the header again\n",
    "                write_header = False\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed {len(chunk_cleaned)} valid rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk: {e}\")\n",
    "\n",
    "    print(f\"Cleaned dataset saved to '{output_file}'\")"
   ],
   "id": "d56c4297e7e31a1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sanitize_filename(filename):\n",
    "    # Replace invalid characters with underscores\n",
    "    return re.sub(r'[<>:\"/\\\\|?*\\s]', '_', filename)\n",
    "\n",
    "def split_type(input_file, output_dir=\"./split\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    written_types = set()\n",
    "    accepted_types = {'fake', 'reliable', 'political', 'bias',\n",
    "                     'conspiracy', 'rumor', 'unreliable', 'clickbait',\n",
    "                     'junksci', 'satire', 'hate', 'unknown'}\n",
    "\n",
    "    chunk_size = 100000\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "        # Clean type values and group\n",
    "        chunk['type'] = chunk['type'].astype(str).str.strip().str.lower()\n",
    "        chunk['type'] = chunk['type'].apply(lambda x: x if x in accepted_types else 'unknown')\n",
    "\n",
    "        for type_val, group in chunk.groupby('type'):\n",
    "            sanitized_type = sanitize_filename(type_val)\n",
    "            file_path = os.path.join(output_dir, f\"{sanitized_type}.csv\")\n",
    "\n",
    "            # Write to file with proper mode/header\n",
    "            mode = 'a' if sanitized_type in written_types else 'w'\n",
    "            header = sanitized_type not in written_types\n",
    "            group.to_csv(file_path, mode=mode, index=False, header=header)\n",
    "\n",
    "            if sanitized_type not in written_types:\n",
    "                written_types.add(sanitized_type)\n",
    "                print(f\"Created: {file_path} (rows: {len(group)})\")\n",
    "            else:\n",
    "                print(f\"Appended: {file_path} (rows: {len(group)})\")\n",
    "\n",
    "    for type_val in accepted_types:\n",
    "        file_path = os.path.join(output_dir, f\"{type_val}.csv\")\n",
    "        print(f\"Total rows in {type_val}: {len(pd.read_csv(file_path))}\")"
   ],
   "id": "2681f43dd591dc73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_combined():\n",
    "    file_paths = [\n",
    "        './split/fake.csv',\n",
    "        './split/reliable.csv',\n",
    "        './split/political.csv',\n",
    "        './split/bias.csv',\n",
    "        './split/conspiracy.csv',\n",
    "        './split/rumor.csv',\n",
    "        './split/unreliable.csv',\n",
    "        './split/junksci.csv',\n",
    "        './split/clickbait.csv'\n",
    "    ]\n",
    "\n",
    "    # Mapping from original labels to merged categories\n",
    "    label_mapping = {\n",
    "        'fake': 'misinformation',\n",
    "        'reliable': 'credible',\n",
    "        'political': 'political_bias',\n",
    "        'bias': 'political_bias',\n",
    "        'conspiracy': 'misinformation',\n",
    "        'rumor': 'misinformation',\n",
    "        'unreliable': 'unreliable',\n",
    "        'junksci': 'misinformation',\n",
    "        'clickbait': 'unreliable'\n",
    "    }\n",
    "\n",
    "    csv.field_size_limit(131072 * 50)\n",
    "    header_tracker = set()\n",
    "\n",
    "    chunk_size = 100_000\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Extract original label from filename\n",
    "            original_label = file_path.split('/')[-1].replace('.csv', '')\n",
    "            new_label = label_mapping.get(original_label, '')\n",
    "            if new_label == '':\n",
    "                continue\n",
    "\n",
    "            # Read and process the chunk\n",
    "            for chunk in pd.read_csv(\n",
    "                file_path,\n",
    "                usecols=['type', 'content', 'title'],\n",
    "                quoting=csv.QUOTE_MINIMAL,\n",
    "                engine='python',\n",
    "                chunksize=chunk_size\n",
    "            ):\n",
    "                # Update the type column with merged category\n",
    "                chunk['type'] = new_label\n",
    "                chunk = shuffle_data(chunk)\n",
    "\n",
    "                write_header = new_label not in header_tracker\n",
    "                if write_header:\n",
    "                    header_tracker.add(new_label)\n",
    "\n",
    "                chunk.to_csv(\n",
    "                    f\"./combined_split/{new_label}.csv\",\n",
    "                    index=False,\n",
    "                    header=write_header,\n",
    "                    mode='a'\n",
    "                )\n",
    "\n",
    "                print(f\"Processed '{file_path}' ({len(chunk)} rows) â†’ '{new_label}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{file_path}': {e}\")\n",
    "            continue"
   ],
   "id": "73c96dc5c9eeef56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_dataset(data_frame, name):\n",
    "    data_frame.to_csv(name, index=False)"
   ],
   "id": "83a56a5c19edc8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_merged_dataset():\n",
    "    output_file = 'news_merged.csv'\n",
    "    file_paths = [\n",
    "        './combined_split/credible.csv',\n",
    "        './combined_split/misinformation.csv',\n",
    "        './combined_split/political_bias.csv',\n",
    "        './combined_split/unreliable.csv'\n",
    "    ]\n",
    "\n",
    "    csv.field_size_limit(131072 * 50)\n",
    "\n",
    "    write_header = True\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        for file_name in file_paths:\n",
    "            try:\n",
    "                # Read the file\n",
    "                chunk = pd.read_csv(file_name, quoting=csv.QUOTE_MINIMAL, engine='python', nrows=300_000, usecols=['type', 'content', 'title'])\n",
    "\n",
    "                shuffle_data(chunk)\n",
    "                # Write the chunk to the output file\n",
    "                chunk.to_csv(f_out, index=False, header=write_header)\n",
    "\n",
    "                # After the first chunk, do not write the header again\n",
    "                write_header = False\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed '{file_name}' with {len(chunk)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing '{file_name}': {e}\")"
   ],
   "id": "e66c72e8d75d2cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)"
   ],
   "id": "101d2d7927b942fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clear_dataset()",
   "id": "25820f92379ac573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "split_type('news_cleaned_valid_records.csv')",
   "id": "6a391fba91741e78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "split_combined()",
   "id": "1ad8feabb5e574ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "create_merged_dataset()",
   "id": "f0787c2ae0931609"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
